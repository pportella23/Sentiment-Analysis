{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Social Media Sentiment Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOEsKgl1zUK1o4bG4nwZ5TI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pportella23/Sentiment-Analysis/blob/master/Social_Media_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC36_bJPvD1T",
        "colab_type": "text"
      },
      "source": [
        "# **Atividade Prática de Análise de Sentimento**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK8Y9PRfCPzQ",
        "colab_type": "text"
      },
      "source": [
        "## **PARTE I**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZWHEeVhkkNh",
        "colab_type": "text"
      },
      "source": [
        "### **Descrição do dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YwcsbsTmy-k",
        "colab_type": "text"
      },
      "source": [
        "Dado um dataset de treino composto por tweets e rótulos, onde rótulo ‘1’ denota que o tweet tem conteúdo negativo e rótulo ‘0’ denota que o tweet tem conteúdo positivo, nosso objetivo é tentar prever os rótulos para um dataset de teste. \n",
        "\n",
        "O dataset no seu formato puro (raw) é composto de:\n",
        "*   id : O id associado com cada tweet no dataset.\n",
        "*   tweets : Os tweets coletados de várias fontes e com sentimentos negativos ou positivos associados à eles.\n",
        "*   Rótulo : Valor binário de representação de sentimento. Sendo ‘0’ se o tweet possui um conteúdo positivo e ‘1’ se o tweet possui um conteúdo negativo.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swbpxj4iqaXN",
        "colab_type": "text"
      },
      "source": [
        "O primeiro passo é importar todos os pacotes necessários para trabalhar com o pré-processamento desses dados coletados no dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIBGPFl7aq0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WS3ihguqwRn",
        "colab_type": "text"
      },
      "source": [
        "Agora vamos efetuar a leitura desses datasets, que estão contidos em um repositório do github como arquivos csv, e salvá-los em cópias dos datasets originais por segurança"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65Xy-6u5q4PR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Leitura do dataset de treino\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv')\n",
        "\n",
        "# Leitura do dataset de teste\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/test.csv')\n",
        "\n",
        "# Cópia do dataset de treino original\n",
        "train_original=train.copy()\n",
        "\n",
        "# Cópia do dataset de teste original\n",
        "test_original=test.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nInr2ObJsk6p",
        "colab_type": "text"
      },
      "source": [
        "É possível obter um overview dos datasets e suas estruturas (conforme explicado no enunciado)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoI8yRUSs5fD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comandos para obter um overview do dataset e averiguar sua estrutura\n",
        "\n",
        "# dataset de treino (3 colunas)\n",
        "train\n",
        "\n",
        "# dataset de teste (2 colunas)\n",
        "test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJQdDWbmtHkv",
        "colab_type": "text"
      },
      "source": [
        "OBS:\n",
        "\n",
        "Note que se você tentar visualizar o dataset de teste (test) só terá duas colunas: id e tweet.\n",
        "\n",
        "Isso se dá, porque ainda não fizemos a predição dos rótulos.\n",
        "\n",
        "Agora, se a visualização for do dataset de treino (train) terá as três colunas comentadas no enunciado: id, tweet e rótulo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDJvVk0_vhQU",
        "colab_type": "text"
      },
      "source": [
        "### **Pré-Processamento dos dados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwCKN4wIv12f",
        "colab_type": "text"
      },
      "source": [
        "Vamos começar combinando os dois datasets por meio de uma anexação de todo o dataset de teste no final do dataset de treino "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6UYqlmGvqLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine = train.append(test,ignore_index=True,sort=True)\n",
        "\n",
        "# Podemos visualizar, para garantir que está correto, o topo e o final do dataset que foi gerado da combinação\n",
        "\n",
        "# O topo (primeiras 5 linhas) do dataset são correspondentes as informações que eram do dataset de treino\n",
        "combine.head()\n",
        "\n",
        "# O final (últimas 5 linhas) do dataset são correspondentes as informações que eram do dataset de teste\n",
        "combine.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRS1xSftxDfM",
        "colab_type": "text"
      },
      "source": [
        "OBS:\n",
        "\n",
        "Perceba que a coluna `label` das linhas finais estão populadas com o valor `NaN`, justamente porque essas linhas são referentes aos dados do dataset de teste que não possui rótulos ainda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41BLb972x6iZ",
        "colab_type": "text"
      },
      "source": [
        "Nosso próximo passo será remover os nomes de usuário dos tweets, visto que não contribuem em nada para a análise "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-osyNJ3OxnNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Para isso criamos uma função que recebe o texto e o que será removido dele\n",
        "\n",
        "def remove_pattern(text,pattern):\n",
        "    \n",
        "    # re.findall() acha o conteúdo a ser removido e coloca em uma lista r\n",
        "\n",
        "    r = re.findall(pattern,text)\n",
        "    \n",
        "    # re.sub() remove o conteúdo das sentenças do\n",
        "    \n",
        "    for i in r:\n",
        "        text = re.sub(i,\"\",text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQhvFuMAy_If",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# E aqui então utilizamos a função recém criada passando os dados que desejamos\n",
        "\n",
        "# Usamos np.vectorize porque é muito mais otimizado quando trata-se de datasets grandes\n",
        "\n",
        "combine['Tidy_Tweets'] = np.vectorize(remove_pattern)(combine['tweet'], \"@[\\w]*\")\n",
        "\n",
        "# E podemos visualizar novamente o dataset, dessa vez com mais uma coluna adicionada\n",
        "\n",
        "combine.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWRcyKgMzvR7",
        "colab_type": "text"
      },
      "source": [
        "Nesse próximo passo iremos remover pontuações, números, caracteres especiais e stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy0-SIjN0EQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remoção das pontuações e caracteres especiais\n",
        "\n",
        "combine['Tidy_Tweets'] = combine['Tidy_Tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "\n",
        "# Remoção das stop words (palavras com menos de 3 letras)\n",
        "\n",
        "combine['Tidy_Tweets'] = combine['Tidy_Tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "\n",
        "# Agora podemos visualizar o dataset com a coluna Tidy atualizada\n",
        "\n",
        "combine.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr7CD3lQ0q4H",
        "colab_type": "text"
      },
      "source": [
        "Outra etapa importante é realizar a tokenização, que seria dividir a sentença em cada uma de suas palavras. Ou seja, cada palavra da sentença receberá uma posição diferente em um vetor criado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8G0dfhq1HxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Comando de tokenização\n",
        "\n",
        "tokenized_tweet = combine['Tidy_Tweets'].apply(lambda x: x.split())\n",
        "\n",
        "# Visualização da lista de vetores com os tweets tokenizados\n",
        "\n",
        "tokenized_tweet.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd0hOIV61lOy",
        "colab_type": "text"
      },
      "source": [
        "Essa tokenização foi realizada justamente para facilitar a etapa de stemming. Nessa etapa iremos remover derivações e formas flexionadas das palavras, pois na maioria das vezes temos muitas palavras com contextos relacionados e que contém significados muito semelhantes (por exemplo: nation, national, nationality). Portanto, desejamos extrair apenas o radical dessas palavras para facilitar o trabalho"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAXo0SXZ2a--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Faremos a importação do método de stemming da lib nltk\n",
        "# nltk é uma lib de PLN muito conhecida do Python\n",
        "\n",
        "from nltk import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x: [ps.stem(i) for i in x])\n",
        "\n",
        "tokenized_tweet.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUTdt-kb3h05",
        "colab_type": "text"
      },
      "source": [
        "Agora que já extraímos os radicais das palavras, vamos juntar esses tokens novamente em sentenças"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBGLFqpc3sXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Um for que passa pelos tokens e vai juntando-os novamente no vetor\n",
        "\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
        "\n",
        "combine['Tidy_Tweets'] = tokenized_tweet\n",
        "\n",
        "combine.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU5ya_m436GS",
        "colab_type": "text"
      },
      "source": [
        "E dessa forma, temos o pré-processamento do nosso dataset realizado! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8INJ6g5b3-gq",
        "colab_type": "text"
      },
      "source": [
        "### **Visualização dos dados**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj3WBfbO415Z",
        "colab_type": "text"
      },
      "source": [
        "Nessa seção falaremos de visualização de dados, um fator muito importante porque nos dá uma ideia aproximada sobre o dataset que tratamos antes de aplicar os modelos de aprendizado de máquina. Falaremos em especial de uma técnica bastante conhecida que é a `WordCloud`, ela nada mais é do que uma visualização onde as palavras que aparecem com maior frequência são representadas com um tamanho maior e as menos frequentes com tamanhos menores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4kODasc52J5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# O Python já possui um pacote bastante utilizado de wordcloud\n",
        "\n",
        "from wordcloud import WordCloud,ImageColorGenerator\n",
        "from PIL import Image\n",
        "import urllib\n",
        "import requests\n",
        "\n",
        "# Basta armazenar as palavras\n",
        "\n",
        "all_words_positive = ' '.join(text for text in combine['Tidy_Tweets'][combine['label']==0])\n",
        "\n",
        "# E criar a \"nuvem\"\n",
        "\n",
        "# Combinando a imagem de fundo com as palavras do dataset\n",
        "\n",
        "Mask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))\n",
        "\n",
        "# Aqui realizamos manipulação da imagem e das palavras com a lib do wordcloud importado\n",
        "\n",
        "image_colors = ImageColorGenerator(Mask)\n",
        "\n",
        "# E agora usamos a função WordCloud para gerar a imagem \n",
        "\n",
        "wc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(all_words_positive)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y7abaJK7MyD",
        "colab_type": "text"
      },
      "source": [
        "E agora vamos exibir a imagem que geramos da \"nuvem\" de palavras positivas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBYFWihB7LLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tamanho da imagem gerada\n",
        "\n",
        "plt.figure(figsize=(10,20))\n",
        "\n",
        "# Aqui nós recolorimos as palavras do dataset com a cor que era da imagem\n",
        "\n",
        "plt.imshow(wc.recolor(color_func=image_colors),interpolation=\"hamming\")\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHsAAHqz7esU",
        "colab_type": "text"
      },
      "source": [
        "Vamos criar também a \"nuvem\" das palavras negativas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bItbB59X6x3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Armazenando as palavras\n",
        "\n",
        "all_words_negative = ' '.join(text for text in combine['Tidy_Tweets'][combine['label']==1])\n",
        "\n",
        "# Combinando a imagem de fundo com as palavras do dataset\n",
        "\n",
        "Mask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))\n",
        "\n",
        "# Aqui realizamos manipulação da imagem e das palavras com a lib do wordcloud importado\n",
        "\n",
        "image_colors = ImageColorGenerator(Mask)\n",
        "\n",
        "# E agora usamos a função WordCloud para gerar a imagem \n",
        "\n",
        "wc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(all_words_negative)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZXsuva870BT",
        "colab_type": "text"
      },
      "source": [
        "E ver como ela fica exibida"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpMJtu-h72fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tamanho da imagem gerada\n",
        "\n",
        "plt.figure(figsize=(10,20))\n",
        "\n",
        "# Aqui nós recolorimos as palavras do dataset com a cor que era da imagem\n",
        "\n",
        "plt.imshow(wc.recolor(color_func=image_colors),interpolation=\"gaussian\")\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgxtBURb8YoP",
        "colab_type": "text"
      },
      "source": [
        "### **Outras manipulações de dados relevantes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rURfeXLa9irt",
        "colab_type": "text"
      },
      "source": [
        "Levando em consideração que extraímos os dados do Twitter temos ainda o impacto causado pelas hashtags, portanto devemos extraí-las e analisá-las"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ELSg2tC9xc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Função que encontra e retorna as hashtags dos tweets\n",
        "\n",
        "def Hashtags_Extract(x):\n",
        "    hashtags=[]\n",
        "    \n",
        "    # Loop over the words in the tweet\n",
        "    for i in x:\n",
        "        ht = re.findall(r'#(\\w+)',i)\n",
        "        hashtags.append(ht)\n",
        "    \n",
        "    return hashtags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sRAeE3w98c_",
        "colab_type": "text"
      },
      "source": [
        "Depois de extrair essas hashtags devemos colocá-las em alguma estrutura de dados (lista)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZS5AdWI-Exo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lista aninhada de hashtags dos tweets positivos\n",
        "\n",
        "ht_positive = Hashtags_Extract(combine['Tidy_Tweets'][combine['label']==0])\n",
        "\n",
        "ht_positive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hNY3lXf-hAu",
        "colab_type": "text"
      },
      "source": [
        "Podemos desaninhá-las para facilitar uma manipulação posterior "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjdCzq1E-lxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Desaninhando a lista\n",
        "\n",
        "ht_positive_unnest = sum(ht_positive,[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_nLzvQ4-ox3",
        "colab_type": "text"
      },
      "source": [
        "E vamos realizar os mesmos passos para os tweets negativos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwj-YDpH-w86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lista aninhada de hashtags dos tweets negativos\n",
        "\n",
        "ht_negative = Hashtags_Extract(combine['Tidy_Tweets'][combine['label']==1])\n",
        "\n",
        "ht_negative"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9FGbJVi-3Zd",
        "colab_type": "text"
      },
      "source": [
        "Desaninhando a lista negativa também"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWZ6DQbt-5pB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Desaninhando a lista\n",
        "\n",
        "ht_negative_unnest = sum(ht_negative,[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI2a1wP4_F_D",
        "colab_type": "text"
      },
      "source": [
        "Por fim, podemos voltar a trabalhar com visualização de dados por meio de gráficos que exibam essas informações coletadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZF3vOg__dt6",
        "colab_type": "text"
      },
      "source": [
        "### **Plotagem de gráficos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cSR5eay_iE6",
        "colab_type": "text"
      },
      "source": [
        "Começaremos plotando um gráfico em barras dos tweets positivos no dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAQmXVla_wri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Contamos a frequência das palavras positivas\n",
        "\n",
        "word_freq_positive = nltk.FreqDist(ht_positive_unnest)\n",
        "\n",
        "word_freq_positive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo8vpOrBABSy",
        "colab_type": "text"
      },
      "source": [
        "Criamos um dataframe para as palavras de uso mais frequente nas hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl5w6WbkAJ3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criando o dataframe de palavras positivas das hashtags\n",
        "\n",
        "df_positive = pd.DataFrame({'Hashtags':list(word_freq_positive.keys()),'Count':list(word_freq_positive.values())})\n",
        "\n",
        "df_positive.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngjsL9GoAQzj",
        "colab_type": "text"
      },
      "source": [
        "E por fim geramos o gráfico com essas informações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSRYjRYFAUfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotagem do gráfico das palavras positivas\n",
        "\n",
        "df_positive_plot = df_positive.nlargest(20,columns='Count')\n",
        "\n",
        "sns.barplot(data=df_positive_plot,y='Hashtags',x='Count')\n",
        "sns.despine()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIgOoKLSAb8S",
        "colab_type": "text"
      },
      "source": [
        "Agora um gráfico em barras dos tweets negativos no dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xvao9DjAgJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Contamos a frequência das palavras negativas\n",
        "\n",
        "word_freq_negative = nltk.FreqDist(ht_negative_unnest)\n",
        "\n",
        "word_freq_negative"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0OYRrafAlYR",
        "colab_type": "text"
      },
      "source": [
        "Criamos um dataframe para as palavras de uso mais frequente nas hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzEFbAh0ArM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Criando o dataframe de palavras negativas das hashtags\n",
        "\n",
        "df_negative = pd.DataFrame({'Hashtags':list(word_freq_negative.keys()),'Count':list(word_freq_negative.values())})\n",
        "\n",
        "df_negative.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVfon33lBNcs",
        "colab_type": "text"
      },
      "source": [
        "Por fim geramos o gráfico com essas informações"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rHysXBSBTpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotagem do gráfico das palavras negativas\n",
        "\n",
        "df_negative_plot = df_negative.nlargest(20,columns='Count') \n",
        "\n",
        "sns.barplot(data=df_negative_plot,y='Hashtags',x='Count')\n",
        "sns.despine()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bugvUjADCXxa",
        "colab_type": "text"
      },
      "source": [
        "## **PARTE II**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JjhwF2PCamx",
        "colab_type": "text"
      },
      "source": [
        "### **Bag-of-words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8Wx196XEF9B",
        "colab_type": "text"
      },
      "source": [
        "É um método para extrair recursos de sentenças de texto e pode ser utilizado para algoritmos de aprendizado de máquina de treinamento. Ele cria um vocabulário de todas as palavras exclusivas que ocorrem em todos as sentenças do conjunto de treinamento (train dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X90FH7OuFCZe",
        "colab_type": "text"
      },
      "source": [
        "Por exemplo, se tivermos duas sentenças:\n",
        "\n",
        "*   S1: He is a lazy boy. She is also lazy.\n",
        "*   S2: Smith is a lazy person.\n",
        "\n",
        "Primeiramente é criado um vocabulário usando as palavras únicas das sentenças\n",
        "\n",
        "[‘He’ , ’She’ , ’lazy’ , ‘boy’ , ‘Smith’ , ’person’]\n",
        "\n",
        "Como podemos ver, a lista não considera “is” , “a” , “also” nesse vocabulário porque eles não são informações relevantemente necessárias para o modelo\n",
        "\n",
        "\n",
        "\n",
        "*   Temos , S=2, N=6\n",
        "*   É criada uma matriz M de tamanho 2X6 representada como essa [tabela](https://drive.google.com/file/d/1sKw_nOxx50nrsb__dzV39vb5ckFVFMRp/view?usp=sharing)\n",
        "\n",
        "Isso é chamado de abordagem por palavras, já que o número de ocorrências e não a sequência ou a ordem das palavras é importante nessa abordagem.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa3qd819E5ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Temos um pacote chamado CountVectorizer para realizar essa tarefa\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "\n",
        "# Matriz do recurso bag-of-words\n",
        "\n",
        "bow = bow_vectorizer.fit_transform(combine['Tidy_Tweets'])\n",
        "\n",
        "df_bow = pd.DataFrame(bow.todense())\n",
        "\n",
        "df_bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCE9SbS3HSn8",
        "colab_type": "text"
      },
      "source": [
        "### **TF-IDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w4vUVkXHWsV",
        "colab_type": "text"
      },
      "source": [
        "Também conhecido como Frequência do termo - frequência inversa do documento é um peso frequentemente usado na recuperação de informações e na mineração de texto. Esse peso é uma medida estatística usada para avaliar a importância de uma palavra para uma sentença em um dataset. A importância aumenta proporcionalmente ao número de vezes que uma palavra aparece na sentença, mas é compensada pela frequência da palavra no dataset. Esse peso é divido em dois termos:\n",
        "\n",
        "**Frequência do termo (TF):**\n",
        "\n",
        "TF = N° de vezes que o termo aparece em uma sentença / total n° de termos na sentença\n",
        "\n",
        "**Frequência inversa do documento (IDF):**\n",
        "\n",
        "IDF = log (n° total de sentenças / n° de sentenças com o termo)\n",
        "\n",
        "**TF-IDF:**\n",
        "\n",
        "TF-IDF = TF * IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_wQUcApJy5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Temos um pacote do Scikit-Learn que executa esse cálculo, o TfidfVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf=TfidfVectorizer(max_df=0.90, min_df=2,max_features=1000,stop_words='english')\n",
        "\n",
        "tfidf_matrix=tfidf.fit_transform(combine['Tidy_Tweets'])\n",
        "\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.todense())\n",
        "\n",
        "df_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aLf-TrOKQNK",
        "colab_type": "text"
      },
      "source": [
        "Estas são as técnicas de incorporação de palavras que usamos em nosso dataset para extração de recursos.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V4OXyT0KTeA",
        "colab_type": "text"
      },
      "source": [
        "### **Separando dataset em treino e validação**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAn6VCYeLm6p",
        "colab_type": "text"
      },
      "source": [
        "Agora temos um dataset com os recursos do modelo bag-of-words (df_bow) e outro dataset com os recursos do modelo TF-IDF (df_tfidf). A primeira tarefa é dividir o dataset em treino e validação para que possamos treinar e testar nosso modelo antes de aplicá-lo para prever dados de teste não vistos e não rotulados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mipTSqkkMxLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Usando recursos do bag-of-words para o set de treinamento\n",
        "\n",
        "train_bow = bow[:31962]\n",
        "\n",
        "train_bow.todense()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M98qsZMyM39b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Usando recursos do TF-IDF para o set de treinamento \n",
        "\n",
        "train_tfidf_matrix = tfidf_matrix[:31962]\n",
        "\n",
        "train_tfidf_matrix.todense()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA-ynGyIOlQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dividindo os dados em set de treinamento e validação\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFGmojQmOwOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recursos do bag-of-words\n",
        "\n",
        "x_train_bow, x_valid_bow, y_train_bow, y_valid_bow = train_test_split(train_bow,train['label'],test_size=0.3,random_state=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHxWYgzySQ-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Recursos do TF-IDF\n",
        "\n",
        "x_train_tfidf, x_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(train_tfidf_matrix,train['label'],test_size=0.3,random_state=17)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GhqyVGtSrOH",
        "colab_type": "text"
      },
      "source": [
        "Terminamos com a separação dos dataset de treino e validação e agora vamos para parte mais esperada de toda atividade! \\0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O7igNGITBfv",
        "colab_type": "text"
      },
      "source": [
        "### **Aplicando modelos de aprendizado de máquina**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSNK-CYDVdh2",
        "colab_type": "text"
      },
      "source": [
        "O problema que nos propusemos a resolver está incluido na categoria de aprendizado de máquina supervisionado. A maioria dos exemplos práticos de aprendizado de máquina serão de aprendizagem supervisionada. Dando uma breve descrição:\n",
        "\n",
        "O aprendizado supervisionado é onde você tem variáveis ​​de entrada (x) e uma variável de saída (Y) e usa um algoritmo para aprender a função de mapeamento da entrada para a saída.\n",
        "\n",
        "Y = f(X)\n",
        "\n",
        "O objetivo é aproximar tão bem a função de mapeamento que, quando você tiver novos dados de entrada (x), poderá prever as variáveis ​​de saída (Y) para esses dados.\n",
        "\n",
        "Problemas de aprendizado supervisionado podem ser agrupados em problema de regressão e classificação. O nosso problema está no grupo de classificação porque temos que determinar o resultado avaliado em positivo ou negativo.\n",
        "\n",
        "Geralmente, usamos modelos diferentes para ver qual melhor se ajusta ao nosso dataset e, em seguida, usamos esse modelo para prever resultados nos dados de teste.\n",
        "\n",
        "Aqui vamos usar 3 modelos diferentes:\n",
        "\n",
        "*   Logistic Regression\n",
        "*   XGBoost\n",
        "*   Decision Trees\n",
        "\n",
        "Utilizaremos F1 Score durante todo processo para avaliar o desempenho do nosso modelo em vez de precisão (Accuracy).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VCtcA7_YUCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importando f1 score\n",
        "\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYTNqGVTY6tJ",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9wsfj0SY9r1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importando o modelo de logistic regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Ajustando o modelo de Logistic Regression\n",
        "\n",
        "Log_Reg = LogisticRegression(random_state=0,solver='lbfgs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMEZOPTAZ3QO",
        "colab_type": "text"
      },
      "source": [
        "Utilização com o recurso de bag-of-words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9UQUO0KZA4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilizando recurso de bag-of-words\n",
        "\n",
        "Log_Reg.fit(x_train_bow,y_train_bow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D30uIEwlZIsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preditando as probabilidades\n",
        "\n",
        "prediction_bow = Log_Reg.predict_proba(x_valid_bow)\n",
        "\n",
        "prediction_bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13gDYmlpZVPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculando o f1 Score\n",
        "\n",
        "# Se a predição é maior que ou igual a 0.3 então 1 se não 0\n",
        "# Onde 0 é para tweets de sentimento positivo e 1 para negativos\n",
        "\n",
        "prediction_int = prediction_bow[:,1]>=0.3\n",
        "\n",
        "# Convertendo os resultados para int\n",
        "\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "prediction_int\n",
        "\n",
        "# Calculando o f1 score\n",
        "log_bow = f1_score(y_valid_bow, prediction_int)\n",
        "\n",
        "log_bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1CicwnUZxVs",
        "colab_type": "text"
      },
      "source": [
        "Utilização com o recurso de TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtMz6KByZ-l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilizando recurso de TF-IDF\n",
        "\n",
        "Log_Reg.fit(x_train_tfidf,y_train_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1MMAuhIaE6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preditando as probabilidades\n",
        "\n",
        "prediction_tfidf = Log_Reg.predict_proba(x_valid_tfidf)\n",
        "\n",
        "prediction_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsg1Y6wZaKBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculando o f1 Score\n",
        "\n",
        "# Se a predição é maior que ou igual a 0.3 então 1 se não 0\n",
        "# Onde 0 é para tweets de sentimento positivo e 1 para negativos\n",
        "\n",
        "prediction_int = prediction_tfidf[:,1]>=0.3\n",
        "\n",
        "# Convertendo os resultados para int\n",
        "\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "prediction_int\n",
        "\n",
        "# Calculando o f1 score\n",
        "\n",
        "log_tfidf = f1_score(y_valid_tfidf, prediction_int)\n",
        "\n",
        "log_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onFmvEoJaVGJ",
        "colab_type": "text"
      },
      "source": [
        "**XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wrBGvDBacBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importando o modelo de XGBoost\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Ajustando o modelo de XGBoost\n",
        "\n",
        "model_bow = XGBClassifier(random_state=22,learning_rate=0.9)\n",
        "model_tfidf = XGBClassifier(random_state=29,learning_rate=0.7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDh61yboaiQ5",
        "colab_type": "text"
      },
      "source": [
        "Utilização com o recurso de bag-of-words: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKaiOKD6a-Gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilizando recurso bag-of-words\n",
        "\n",
        "model_bow.fit(x_train_bow, y_train_bow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKbEiEesblwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preditando as probabilidades\n",
        "\n",
        "xgb = model_bow.predict_proba(x_valid_bow)\n",
        "\n",
        "xgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xomS4VRlbqmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculando o f1 Score\n",
        "\n",
        "# Se a predição é maior que ou igual a 0.3 então 1 se não 0\n",
        "# Onde 0 é para tweets de sentimento positivo e 1 para negativos\n",
        "\n",
        "xgb=xgb[:,1]>=0.3\n",
        "\n",
        "# Convertendo os resultados para int\n",
        "\n",
        "xgb_int=xgb.astype(np.int)\n",
        "\n",
        "# Calculando o f1 score\n",
        "\n",
        "xgb_bow=f1_score(y_valid_bow,xgb_int)\n",
        "\n",
        "xgb_bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CZjBN5-JcDDU"
      },
      "source": [
        "Utilização com o recurso de TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdmpHiFxeiah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilizando recurso TF-IDF\n",
        "\n",
        "model_tfidf.fit(x_train_tfidf, y_train_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frKmrsbqel_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preditando as probabilidades\n",
        "\n",
        "xgb_tfidf=model_tfidf.predict_proba(x_valid_tfidf)\n",
        "\n",
        "xgb_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2hXWGvWemXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculando o f1 Score\n",
        "\n",
        "# Se a predição é maior que ou igual a 0.3 então 1 se não 0\n",
        "# Onde 0 é para tweets de sentimento positivo e 1 para negativos\n",
        "\n",
        "xgb_tfidf=xgb_tfidf[:,1]>=0.3\n",
        "\n",
        "# Convertendo os resultados para int\n",
        "\n",
        "xgb_int_tfidf=xgb_tfidf.astype(np.int)\n",
        "\n",
        "# Calculando o f1 score\n",
        "\n",
        "score=f1_score(y_valid_tfidf,xgb_int_tfidf)\n",
        "\n",
        "score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zLZLuLqfB0B",
        "colab_type": "text"
      },
      "source": [
        "**Decision Trees**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQB5ZFoJfFyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importando o modelo de Decision Trees\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Ajustando o modelo de Decision Tree\n",
        "\n",
        "dct = DecisionTreeClassifier(criterion='entropy', random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP5bdcDjfxbQ",
        "colab_type": "text"
      },
      "source": [
        "Utilização com o recurso de bag-of-words: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tpPqNT-fuci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilizando recurso bag-of-words\n",
        "\n",
        "dct.fit(x_train_bow,y_train_bow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PRb8sHkf44G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preditando as probabilidades\n",
        "\n",
        "dct_bow = dct.predict_proba(x_valid_bow)\n",
        "\n",
        "dct_bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrP3T6XDf7jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculando o f1 Score\n",
        "\n",
        "# Se a predição é maior que ou igual a 0.3 então 1 se não 0\n",
        "# Onde 0 é para tweets de sentimento positivo e 1 para negativos\n",
        "\n",
        "dct_bow=dct_bow[:,1]>=0.3\n",
        "\n",
        "# Convertendo os resultados para int\n",
        "\n",
        "dct_int_bow=dct_bow.astype(np.int)\n",
        "\n",
        "# Calculando o f1 score\n",
        "\n",
        "dct_score_bow=f1_score(y_valid_bow,dct_int_bow)\n",
        "\n",
        "dct_score_bow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp8gzST9gIt7",
        "colab_type": "text"
      },
      "source": [
        "Utilização com o recurso de TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrLfRTLJgJTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utilizando recurso TF-IDF\n",
        "\n",
        "dct.fit(x_train_tfidf,y_train_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr-caNRigNCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preditando as probabilidades\n",
        "\n",
        "dct_tfidf = dct.predict_proba(x_valid_tfidf)\n",
        "\n",
        "dct_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S412ihmRgPKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculando o f1 Score\n",
        "\n",
        "# Se a predição é maior que ou igual a 0.3 então 1 se não 0\n",
        "# Onde 0 é para tweets de sentimento positivo e 1 para negativos\n",
        "\n",
        "dct_tfidf=dct_tfidf[:,1]>=0.3\n",
        "\n",
        "# Convertendo os resultados para int\n",
        "dct_int_tfidf=dct_tfidf.astype(np.int)\n",
        "\n",
        "# Calculando o f1 score\n",
        "dct_score_tfidf=f1_score(y_valid_tfidf,dct_int_tfidf)\n",
        "\n",
        "dct_score_tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4EtIa_4gcwC",
        "colab_type": "text"
      },
      "source": [
        "### **Comparando modelos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDeUAtqWgm_Z",
        "colab_type": "text"
      },
      "source": [
        "Agora, vamos comparar os diferentes modelos que aplicamos em nosso conjunto de dados com diferentes técnicas de incorporação de palavras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7UoDb-PgqIf",
        "colab_type": "text"
      },
      "source": [
        "**Bag-of-Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWm3AKF5gfCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Algo_1 = ['LogisticRegression(Bag-of-Words)','XGBoost(Bag-of-Words)','DecisionTree(Bag-of-Words)']\n",
        "\n",
        "score_1 = [log_bow,xgb_bow,dct_score_bow]\n",
        "\n",
        "compare_1 = pd.DataFrame({'Model':Algo_1,'F1_Score':score_1},index=[i for i in range(1,4)])\n",
        "\n",
        "compare_1.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAN3CKobg3q-",
        "colab_type": "text"
      },
      "source": [
        "Gerando gráfico de comparação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ynq_O4Rg6AH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(18,5))\n",
        "\n",
        "sns.pointplot(x='Model',y='F1_Score',data=compare_1)\n",
        "\n",
        "plt.title('Bag-of-Words')\n",
        "plt.xlabel('MODEL')\n",
        "plt.ylabel('SCORE')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q8ZVPidg8Pc",
        "colab_type": "text"
      },
      "source": [
        "**TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkiT5JRig-Zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Algo_2 = ['LogisticRegression(TF-IDF)','XGBoost(TF-IDF)','DecisionTree(TF-IDF)']\n",
        "\n",
        "score_2 = [log_tfidf,score,dct_score_tfidf]\n",
        "\n",
        "compare_2 = pd.DataFrame({'Model':Algo_2,'F1_Score':score_2},index=[i for i in range(1,4)])\n",
        "\n",
        "compare_2.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfrOMvhthAok",
        "colab_type": "text"
      },
      "source": [
        "Gerando gráfico de comparação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q2h8BumhA-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(18,5))\n",
        "\n",
        "sns.pointplot(x='Model',y='F1_Score',data=compare_2)\n",
        "\n",
        "plt.title('TF-IDF')\n",
        "plt.xlabel('MODEL')\n",
        "plt.ylabel('SCORE')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9H4NRDshKdX",
        "colab_type": "text"
      },
      "source": [
        "Como podemos ver, o melhor modelo possível de Bag-of-Words e TF-IDF é Logistic Regression. Agora, vamos comparar a pontuação do modelo de Logistic Regression com as técnicas de extração de recursos que são Bag-of-Words e TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_dPlbB3hXOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Algo_best = ['LogisticRegression(Bag-of-Words)','LogisticRegression(TF-IDF)']\n",
        "\n",
        "score_best = [log_bow,log_tfidf]\n",
        "\n",
        "compare_best = pd.DataFrame({'Model':Algo_best,'F1_Score':score_best},index=[i for i in range(1,3)])\n",
        "\n",
        "compare_best.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL4HHfR4hb_i",
        "colab_type": "text"
      },
      "source": [
        "Gerando gráfico de comparação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc0q_OGmhc_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(18,5))\n",
        "\n",
        "sns.pointplot(x='Model',y='F1_Score',data=compare_best)\n",
        "\n",
        "plt.title('Logistic Regression(Bag-of-Words & TF-IDF)')\n",
        "plt.xlabel('MODEL')\n",
        "plt.ylabel('SCORE')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYxieE5pi5zD",
        "colab_type": "text"
      },
      "source": [
        "Fica claro então que a melhor possibilidade para o F1 Score é utilizando o modelo Logistic Regression com o recurso de TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDhFQxIJjEm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tfidf = tfidf_matrix[31962:]\n",
        "test_pred = Log_Reg.predict_proba(test_tfidf)\n",
        "\n",
        "test_pred_int = test_pred[:,1] >= 0.3\n",
        "test_pred_int = test_pred_int.astype(np.int)\n",
        "\n",
        "test['label'] = test_pred_int\n",
        "\n",
        "submission = test[['id','label']]\n",
        "submission.to_csv('result.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGWDw8iNjGXs",
        "colab_type": "text"
      },
      "source": [
        "**Resultado da predição**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbEOtlf-jJFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = pd.read_csv('result.csv')res"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}